{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Python & the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Today, you'll get a chance in groups to build a larger project which uses the `requests` package!\n",
    "\n",
    "Download the starter code for this project at this link: [https://stanfordpython.com/res/starter-code/lab-5.zip](https://stanfordpython.com/res/starter-code/lab-5.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "As a reminder, the packages discussed in class are listed below along with some useful links. Read through the ones that you would like to learn more about, or jump straight into building a Reddit Wallpaper Scraper below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Packages\n",
    "\n",
    "* requests: [Quick Start](http://docs.python-requests.org/en/master/user/quickstart/)\n",
    "* scikit-learn: [Tutorial](https://scikit-learn.org/stable/tutorial/)\n",
    "* nltk: [Overview](http://www.nltk.org/) + [API Docs](http://www.nltk.org/api/nltk.html)\n",
    "* pycodestyle: [Intro](https://pep8.readthedocs.io/en/latest/intro.html)\n",
    "* autopep8: [Overview](https://pypi.org/project/autopep8/)\n",
    "* flask: [Quick Start](http://flask.pocoo.org/docs/quickstart/) + [Mega-Tutorial](http://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world)\n",
    "* django: [Overview](https://www.djangoproject.com/start/overview/) + [Quick Start](https://www.djangoproject.com/start/) + [Tutorial](https://docs.djangoproject.com/en/2.1/intro/tutorial01/)\n",
    "* We'll see more about these in the upcoming weeks:\n",
    "    * numpy: [Quick Start](https://docs.scipy.org/doc/numpy/user/quickstart.html)\n",
    "    * scipy: [Reference](https://docs.scipy.org/doc/scipy/reference/index.html)\n",
    "    * matplotlib/pyplot: [PyPlot Tutorial](https://matplotlib.org/users/pyplot_tutorial.html) + [Gallery](https://matplotlib.org/gallery.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Packages I Like\n",
    "\n",
    "* Pillow - better image processing\n",
    "* twisted - event-driven network programming framework\n",
    "* selenium - Automated web client \n",
    "* dateutil - better date/time handling\n",
    "* SQLAlchemy - Build objects on top of database schemas\n",
    "* bcrypt - Cryptography suite\n",
    "* scrapy - Web scraping\n",
    "* pandas - Vectorized data manipulation\n",
    "* boto - Python interface to Amazon Web Services.\n",
    "* PyEphem - Track planets and satellites.\n",
    "* Basemap - Plot 2D data on maps.\n",
    "* praw - Play with reddit's API, write a reddit bot.\n",
    "* robobrowser - automate web tasks\n",
    "* mechanize - automate web tasks\n",
    "* networkx - visualize small graphs\n",
    "* BeautifulSoup - rapidly prototype HTML parsers\n",
    "* pygame and pyglet - prototyping games\n",
    "* tornado - web framework and asynchronous web framework\n",
    "* cyclone - web server framework connecting tornado and twisted\n",
    "* fabric - Automate interactions with remote servers.\n",
    "* Scrapy - Web scraping/Web crawling framework\n",
    "* Pattern - Web mining\n",
    "* rstr and exrex - Random string generation/regex reversing.\n",
    "* autobahn - web sockets\n",
    "* peewee - simple ORM for databases\n",
    "* sh - Python bindings for unix commands\n",
    "* celery - asynchronous task queue\n",
    "* py2app / py2exe - create standalone Mac OS X app / windows executable\n",
    "* delorean - friendly dates/times\n",
    "* docutils - documentation utilities\n",
    "* colorama - terminal colors\n",
    "* wxpython and pyqt and pygtk - gui toolkits for python\n",
    "* scapy - packet sniffer and analyzer\n",
    "* pywin32 - simplifies common interactions with Windows\n",
    "* nose - testing framework\n",
    "* SymPy - symbolic manipulation including algebraic evaluation, differentiation, expansion\n",
    "* whoosh - full text indexing, search, and spell checking\n",
    "* fuzzywuzzy - fuzzy string matching library\n",
    "* progressbar - easy ASCII progress bars\n",
    "* retrying - easily retry failed operations\n",
    "* phonenumbers - easy to work with phone numbers\n",
    "* gensim - feature vectors for humans\n",
    "* tensorflow - Google's machine intelligence\n",
    "* pytorch - deep learning made easier\n",
    "* sphinx - documentation generation\n",
    "\n",
    "For more, check out [awesome-python](https://awesome-python.com/), which is a \"curated list of awesome Python frameworks, libraries, software and resources.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing\n",
    "\n",
    "With the rest of the time in lab, we're going to get started on a wallpaper scraper! This was an old assignment in CS41 that we've repurposed for this lab. *Note*: If you need it to, this lab can count as your final project but we encourage you to use Python for something you're passionate about if you can. If you're using this lab as a final project, **you need to implement at least two extensions** (see below for more details).\n",
    "\n",
    "You can also spend lab time searching for an open-source project to which to contribute or working on Assignment 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With &#129412; by @psarin, @coopermj, @sredmond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wallscraper\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "Congratulations on (almost) completing Week 5! Midterm season is getting underway and we've almost finished discussing the syntax of the Python language. At this point, you know most of the important stuff about the language itself. Therefore, we'll spend most of the rest of our time here going over third party packages. However, as far as the language itself goes, you have all become pretty skilled in the art of the Python language.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Sigh... another CS41 lab day and boring activity. Better open up reddit and see what's new on [/r/funny](https://www.reddit.com/r/funny).\n",
    "\n",
    "**PSYCH THIS IS THE MOST AMAZING LAB EVER.**\n",
    "\n",
    "While you're aimlessly tabbing between windows, you realize you really need a new desktop background wallpaper. So, you head on over to [/r/wallpapers](https://www.reddit.com/r/wallpapers) and maybe peruse [/r/wallpaper](https://www.reddit.com/r/wallpaper) as well. If you're feeling up for it, you might check out [/r/earthporn](https://www.reddit.com/r/earthporn) and [/r/spaceporn](https://www.reddit.com/r/spaceporn).\n",
    "\n",
    "Generally, these labs have focused on exploring nuances of the Python language - whether the syntax, semantics, or style of thinking. However, since we've now almost wrapped up talking about the language, labs will become a period of time for you to build something awesome.\n",
    "\n",
    "In particular, today you will write a program that automatically downloads the top wallpaper from reddit every night to your local computer, and optionally sets it as your desktop background. So cool!\n",
    "\n",
    "## Getting Set Up\n",
    "\n",
    "While Python's standard library has a lot of functionality included, we sometimes prefer to work with third-party packages. For this project, we're going to primarily use `requests`, a fantastic web client for Python written by Kenneth Reitz.\n",
    "\n",
    "### Installing Required Packages\n",
    "\n",
    "As always, the first step is installing any required packages using `pip`. At the very least, you should ensure that `requests` is installed inside your virtual environment:\n",
    "\n",
    "```\n",
    "# Activate your virtual environment\n",
    "$ workon cs41-env\n",
    "(cs41-env)$ pip install requests\n",
    "```\n",
    "\n",
    "You are free to install any other third-party libraries you think will be useful. In particular, `awesome-slugify` can be used to normalize possibly complicated filenames, and `Pillow` can simplify operations on images.\n",
    "\n",
    "Our solution also uses the `os`, `sys`, `io`, `subprocess`, `pathlib`, `imghdr`, and `mimetypes` packages from the standard library.\n",
    "\n",
    "### Check Installations\n",
    "To ensure that you've successfully installed requests, run:\n",
    "\n",
    "```\n",
    "(cs41-env)$ python -c \"import requests\"\n",
    "(cs41-env)$\n",
    "```\n",
    "\n",
    "If you get an error that looks like `ModuleNotFoundError: No module named 'requests'`, then you have not installed requests in your virtual environment.\n",
    "\n",
    "## Starter Code\n",
    "\n",
    "```\n",
    "lab-5/\n",
    "├── wallscraper-notebook.ipynb\n",
    "├── wallpapers/\n",
    "├── wallscraperutils.py\n",
    "└── wallscraper.py\n",
    "```\n",
    "\n",
    "In addition to this notebook, the other starter files are:\n",
    "\n",
    "* `wallpapers/`: where all the downloaded wallpapers will go.\n",
    "* `wallscraper.py`: Barebones starter code. All of your program logic will go into this file.\n",
    "* `wallscraperutils.py`: A few helper functions that may simplify some of the less interesting steps of the assignment. Read through the file for more information.\n",
    "\n",
    "## Wallscraper Specification\n",
    "\n",
    "The internet is full of many awesome things: [cat videos](https://www.youtube.com/v/2XID_W4neJo), [the most awesome person in the world](https://www.facebook.com/), and most importantly, [reddit](https://www.reddit.com/r/python).\n",
    "\n",
    "Take a (brief) look at [reddit.com/r/wallpapers](https://www.reddit.com/r/wallpapers) - there is a lot happening on that page. Images are dynamically loaded, buttons ask you to click them, ads on the side demand your attention - it can be hard to find the data, and although we could use something like `BeautifulSoup` to parse all this junk, it seems almost too complicated to be worth it.\n",
    "\n",
    "On the other hand, take a (longer) look at [reddit.com/r/wallpapers.json](https://www.reddit.com/r/wallpapers.json) (note the suffix `.json`). By adding this suffix to the query, we get back a rich data structure representing, in this case, posts on /r/wallpapers.\n",
    "\n",
    "### Overview\n",
    "\n",
    "At a high level, you will need to extract a list of the top posts from a subreddit, and for each of the posts, download the linked image to your computer if it represents a (SFW) image.\n",
    "\n",
    "### Aside: Using `requests`\n",
    "\n",
    "In this section, we'll explore some of the functionality of the `requests` module, which will be quite useful for this assignment. You can skip this if you already know are comfortable with how `requests` works.\n",
    "\n",
    "A sample usage of the `requests` package is shown below.\n",
    "\n",
    "```python\n",
    ">>> import requests\n",
    ">>> response = requests.get('https://stanfordpython.com')\n",
    ">>> print(response)\n",
    "<Response [200]>\n",
    ">>> print(type(response))\n",
    "<class 'requests.models.Response'>\n",
    "```\n",
    "\n",
    "The `requests.get` function returns a `Response` object that represents the response returned by the server, in this case by `stanfordpython.com`. (There are similar `post`, `put`, `patch`, and `delete` functions defined by `requests`).\n",
    "\n",
    "A `Response` instance supports a lot of attribute references:\n",
    "\n",
    "```\n",
    ">>> response.<tab>\n",
    "response.apparent_encoding      response.history                response.ok\n",
    "response.close                  response.is_permanent_redirect  response.raise_for_status\n",
    "response.connection             response.is_redirect            response.raw\n",
    "response.content                response.iter_content           response.reason\n",
    "response.cookies                response.iter_lines             response.request\n",
    "response.elapsed                response.json                   response.status_code\n",
    "response.encoding               response.links                  response.text\n",
    "response.headers                response.next                   response.url\n",
    "```\n",
    "\n",
    "For this project, we only care about a few of these:\n",
    "\n",
    "```python\n",
    "# True if and only if the server returned a successful response.\n",
    "response.ok\n",
    "\n",
    "# The raw server response, as a bytes object.\n",
    "response.content\n",
    "\n",
    "# A Python dictionary containing the response data if the response represents JSON-encoded data.\n",
    "# If the response isn't JSON data, raise an Exception.\n",
    "response.json()\n",
    "```\n",
    "\n",
    "More information on the `requests` library can be found [here](http://docs.python-requests.org/en/latest/).\n",
    "\n",
    "### Query Subreddit Data\n",
    "\n",
    "In this section, your task is to write a function `query` that accepts as an argument a subreddit to query (e.g. `'wallpapers'` or `'funny+gifs'`), and returns the JSON server response from reddit as a Python dictionary. You can add any additional positional or keyword arguments as you see fit.\n",
    "\n",
    "Your function should gracefully handle all of the following scenarios:\n",
    "\n",
    "* There is no internet connection\n",
    "* The user supplies a string that doesn't represent a valid subreddit\n",
    "* The requests module, in particular the `get` function, throws any exception from `requests.exceptions` (hint: look through [the source code](https://github.com/kennethreitz/requests/blob/master/requests/exceptions.py) to find the base exception class for the `requests` package.)\n",
    "* reddit responds with a status that is not `ok`\n",
    "\n",
    "In all of these situations, your `query` function should print out an informative error message.\n",
    "\n",
    "To test out this function, write a few lines of code to query a reasonable subreddit and compute the number of posts with a score greater than 500.\n",
    "\n",
    "#### Note: Rate Limits\n",
    "\n",
    "Reddit imposes a rate limit on generic scripts that make too many requests to its server (default is >30 per minute). If your script is getting rate-limited, Reddit will respond with a `<Response [429]>`, which specifically means: *429 Client Error: Too Many Requests.*\n",
    "\n",
    "To avoid this problem, we need to tell Reddit that we're not just some random script by adding a `User-Agent` to our request. In particular, you need to add `headers={'User-Agent': <unique_identifier>}` as a keyword argument to `requests.get`.\n",
    "\n",
    "For instance, I might use \n",
    "\n",
    "```python\n",
    "requests.get(\n",
    "    'http://www.website.com', \n",
    "    headers={'User-Agent': 'Wallscraper Script by @psarin'}\n",
    ")\n",
    "```\n",
    "\n",
    "This should avoid the rate-limit problem for class, but is not to be abused!\n",
    "\n",
    "### Building a `RedditPost` Class\n",
    "\n",
    "Next, you should build a `RedditPost` class that represents a single post.\n",
    "\n",
    "A RedditPost object must support two methods, and can support as many helper functions as you see fit.\n",
    "\n",
    "* `__init__(self, data)`: Initialize a RedditPost from a JSON dictionary representing the post, as extracted from the top-level subreddit JSON.\n",
    "* `download(self)`: Tries to download the Reddit post. Must determine (1) if the post can be downloaded, (2) where to download the post, and (3) actually download the post.\n",
    "\n",
    "For now, let's focus on the constructor. As discussed in the data model section later on, there are lots of irrelevant attributes in the JSON returned by the subreddit.\n",
    "\n",
    "Write the `__init__` method, and only keep the attributes that correspond to an attribute you think will be useful. If an attribute is missing or the data is otherwise corrupted, your program should handle the error gracefully.\n",
    "\n",
    "Additionally, you can implement the magic method `__str__(self)` to return a string representing a human-readable form of a post, allowing us to more easily debug when printing a `RedditPost` to the console. We suggest printing the posts in the following format: `\"RedditPost({title} ({score}): {url})\"`. **We haven't discussed magic methods yet**; Michael will talk about them more next week, but you should investigate and try to implement it if you're interested.\n",
    "\n",
    "You're also welcome to change this API if you'd like.\n",
    "\n",
    "### Load Response Data into Post Objects\n",
    "\n",
    "Write the code to convert the data returned by `query` into a list of `RedditPost` objects. You can accomplish this in one line using a list comprehension. As a sanity check, your list of `RedditPost`s should have length 25 (or perhaps 26 or 27, depending on stickied posts).\n",
    "\n",
    "If the data is bad - i.e. keys are missing, information is not structured as you suspect, etc. - your program should not crash. Rather, it should gracefully handle the errors and proceed accordingly. Which function's responsibility should it be to check for malformed data?\n",
    "\n",
    "At this point, rewrite your old code to determine the number of posts with a score greater than 500. This can also be done in one line of code.\n",
    "\n",
    "### Download an Image Post\n",
    "\n",
    "Ultimately, our goal is to download wallpapers. Implement the `self.download()` method in the `RedditPost` class that attempts to download the post.\n",
    "\n",
    "If the post doesn't represent an image, don't download anything. How can you tell if the post represents an image? You can look at the `url` - does it end with `'.jpg'`,`'.png'`. or any other image suffix? Is the `is_self` attribute `True` or `False`? Is the `post_hint` attribute `image`, `link`, or something else? Is the domain something recognizable like `'i.imgur.com'`?\n",
    "\n",
    "You can add any other conditions you'd like on the downloaded wallpapers - perhaps you only download images from imgur, or only wallpapers with a score over 500, or only gilded posts.\n",
    "\n",
    "Where do you download the file to? Use the aspect ratio and width/height to store the download in a structured place. For example, if an image is 1920 by 1080, store it in `wallpapers/16x9/1920x1080/image.png`. How can you title the file? For one, you can use the title of the post. However, sometimes Reddit posts have titles that aren't amenable to filesystems, so you should probably `slugify` the title in some way. Furthermore, most titles have something like `'[1920x1080]'` in the title. You should use a regular expression to detect and remove anything that looks like that, possibly using `re.sub`.\n",
    "\n",
    "Hint: if you're writing image data to an open file object, make sure that the file has been opened with `wb` flags for (w)riting in (b)inary mode. Generally, when reading or writing binary data such as images or sound files, it's a good idea to use the `'b'` option.\n",
    "\n",
    "Test this method by downloading one of the posts.\n",
    "\n",
    "If you have successfully downloaded a photo, congratulations! That's pretty dang impressive.\n",
    "\n",
    "### Tying Everything Together\n",
    "\n",
    "Ultimately, the goal of this step is to combine all of the pieces you've already written to complete the final project.\n",
    "\n",
    "Write the code to take the list of posts generated earlier, and download them all to your filesystem. Cool!\n",
    "\n",
    "## Data Model\n",
    "\n",
    "Building this wallpaper scraper involves scraping structured data from Reddit. How exactly is this data structured?\n",
    "\n",
    "### Top-Level Subreddit Data - `listing`\n",
    "\n",
    "The data returned by a subreddit is a `listing` object, which is used to paginate content that is too long to display in one go.\n",
    "\n",
    "```python\n",
    "{'data': {'after': 't3_4if6xu',\n",
    "          'before': None,\n",
    "          'children': [list of Things],\n",
    "          'modhash': ''},\n",
    " 'kind': 'Listing'}\n",
    "```\n",
    "\n",
    "Where `'children'` is a list of `Thing`s (the real Reddit name for this data model!)\n",
    "\n",
    "If you want to get the previous or next page, supply a query argument `before` or `after` with the value, usually used in conjunction with `count`.\n",
    "\n",
    "### Intermediate Storage - `Thing`\n",
    "\n",
    "A `Thing`, for our purposes, looks like the following:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'data': Post,\n",
    "    'kind': 't3'\n",
    "}\n",
    "```\n",
    "\n",
    "where the only important field, `'data'`, contains a single `Post` object.\n",
    "\n",
    "### Reddit Post - `Post`\n",
    "\n",
    "The most important data model to understand is that of a Reddit post. In it's entirety, a post looks like:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'approved_by': None,\n",
    "    'archived': True,\n",
    "    'author': 'onewallpaperaweek',\n",
    "    'author_flair_css_class': None,\n",
    "    'author_flair_text': None,\n",
    "    'banned_by': None,\n",
    "    'clicked': False,\n",
    "    'created': 1431645046.0,\n",
    "    'created_utc': 1431616246.0,\n",
    "    'distinguished': None,\n",
    "    'domain': 'i.imgur.com',\n",
    "    'downs': 0,\n",
    "    'edited': False,\n",
    "    'from': None,\n",
    "    'from_id': None,\n",
    "    'from_kind': None,\n",
    "    'gilded': 0,\n",
    "    'hidden': False,\n",
    "    'hide_score': False,\n",
    "    'id': '35ybkb',\n",
    "    'is_self': False,\n",
    "    'likes': None,\n",
    "    'link_flair_css_class': None,\n",
    "    'link_flair_text': None,\n",
    "    'locked': False,\n",
    "    'media': None,\n",
    "    'media_embed': {},\n",
    "    'mod_reports': [],\n",
    "    'name': 't3_35ybkb',\n",
    "    'num_comments': 27,\n",
    "    'num_reports': None,\n",
    "    'over_18': False,\n",
    "    'permalink': '/r/wallpaper/comments/35ybkb/its_a_misty_mood_sort_of_day_1920x1080/',\n",
    "    'post_hint': 'image',\n",
    "    'preview': {\n",
    "        'images': [{\n",
    "            'id': '_8zF29cGX1DwJ0KDnbYGbh2oycytb6RQS1d807LC898',\n",
    "            'resolutions': [{\n",
    "                'height': 60,\n",
    "                'url': 'https://i.redditmedia.com/jwI5mvqJE-Cx1C5S99XP-RSB6B3TJKVJr-KKTVmb2zg.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=43747ff659df46f3a8cbd0699b3fc2ec',\n",
    "                'width': 108\n",
    "            }, {\n",
    "                'height': 121,\n",
    "                'url': 'https://i.redditmedia.com/jwI5mvqJE-Cx1C5S99XP-RSB6B3TJKVJr-KKTVmb2zg.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=5a7b8c50f90f08cf38121bfbbb518cc2',\n",
    "                'width': 216\n",
    "            }, {\n",
    "                'height': 179,\n",
    "                'url': 'https://i.redditmedia.com/jwI5mvqJE-Cx1C5S99XP-RSB6B3TJKVJr-KKTVmb2zg.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=dddde0d7b2389824adf43cae298bcd92',\n",
    "                'width': 320\n",
    "            }, {\n",
    "                'height': 359,\n",
    "                'url': 'https://i.redditmedia.com/jwI5mvqJE-Cx1C5S99XP-RSB6B3TJKVJr-KKTVmb2zg.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=640&amp;s=c2a05cd908a84f0261dafe2b99b70a8a',\n",
    "                'width': 640\n",
    "            }, {\n",
    "                'height': 539,\n",
    "                'url': 'https://i.redditmedia.com/jwI5mvqJE-Cx1C5S99XP-RSB6B3TJKVJr-KKTVmb2zg.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=960&amp;s=f9112c9b8a95cf8a9399bd4c049a510e',\n",
    "                'width': 960\n",
    "            }, {\n",
    "                'height': 607,\n",
    "                'url': 'https://i.redditmedia.com/jwI5mvqJE-Cx1C5S99XP-RSB6B3TJKVJr-KKTVmb2zg.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=1080&amp;s=95913eda85a710c8e753cc15f498c7e2',\n",
    "                'width': 1080\n",
    "            }],\n",
    "            'source': {\n",
    "                'height': 1079,\n",
    "                'url': 'https://i.redditmedia.com/jwI5mvqJE-Cx1C5S99XP-RSB6B3TJKVJr-KKTVmb2zg.jpg?s=c69cfcbf626335086ae4273a6b54b45e',\n",
    "                'width': 1919\n",
    "            },\n",
    "            'variants': {}\n",
    "        }]\n",
    "    },\n",
    "    'quarantine': False,\n",
    "    'removal_reason': None,\n",
    "    'report_reasons': None,\n",
    "    'saved': False,\n",
    "    'score': 833,\n",
    "    'secure_media': None,\n",
    "    'secure_media_embed': {},\n",
    "    'selftext': '',\n",
    "    'selftext_html': None,\n",
    "    'stickied': False,\n",
    "    'subreddit': 'wallpaper',\n",
    "    'subreddit_id': 't5_2qmjl',\n",
    "    'suggested_sort': None,\n",
    "    'thumbnail': 'http://a.thumbs.redditmedia.com/VJxDvwX98DdVVckX5-bXrO6gmoh7oHCHPBLIfyjvRn4.jpg',\n",
    "    'title': \"It's a Misty Mood sort of day [1920x1080]\",\n",
    "    'ups': 833,\n",
    "    'url': 'http://i.imgur.com/fWbnJYt.jpg',\n",
    "    'user_reports': [],\n",
    "    'visited': False\n",
    "}\n",
    "```\n",
    "\n",
    "That's quite a lot of information! Much of this information isn't relevant to our purposes. For this assignment, you should keep only the following attributes:\n",
    "\n",
    "```\n",
    "subreddit - which subreddit this post originated from\n",
    "is_self - True iff the post is a self-, text-only post\n",
    "ups - number of upvotes\n",
    "post_hint - reddit's guess of the content of the post (could be 'image', 'link', or something else.)\n",
    "title - title of the post\n",
    "downs - number of downvotes \n",
    "score - the overall score of the post (basically ups - downs, but with \"vote fuzzing\")\n",
    "url - the post's link, if it is not a self post\n",
    "domain - the domain of the url \n",
    "permalink - a permanent link to the reddit post\n",
    "created_utc - epoch timestamp in UTC of the post's creation\n",
    "num_comments - how many comments the post has\n",
    "preview - data structure containing image previews\n",
    "name - unique name for this post\n",
    "over_18 - true iff the post is not safe for work (NSFW)\n",
    "```\n",
    "\n",
    "You will find that some of these attributes are more helpful than others.\n",
    "\n",
    "### Extras\n",
    "\n",
    "#### Learning More\n",
    "\n",
    "For the full description of Reddit JSON objects, check out [the documentation](https://github.com/reddit/reddit/wiki/JSON).\n",
    "\n",
    "#### Viewing JSON Data In-Browser\n",
    "\n",
    "If you're planning to poke around sample JSON data from the browser, I highly recommend JSONView for [Chrome](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc) and [Firefox](https://addons.mozilla.org/en-us/firefox/addon/jsonview/). This browser addition makes it easy to explore the structure of JSON from the browser. Unfortunately, there isn't a good equivalent tool for Safari.\n",
    "\n",
    "## Pythonic Suggestions\n",
    "\n",
    "When processing the data from a given subreddit, make use of list comprehensions to simplify your data exploration. For example, you should never need to build an empty list during any part of this project.\n",
    "\n",
    "If you pass `stream=True` as a keyword argument to `requests.get`, the `.content` will not be loaded at once into memory. Instead, you can use `requests.iter_content(chunk_size=1024)` to iterate over the server response content. This is generally considered good practice, and should be used when downloading image files, which may be arbitrarily large.\n",
    "\n",
    "In keeping with the motto of \"coding for the common case\", you should generally blindly assume that your data is properly formatted, and catch any improper behavior in an `except` block. That is, use exceptional control flow to simplify error handling.\n",
    "\n",
    "## Extensions\n",
    "\n",
    "Some of these are easy, some of these are very hard. If you want to use this as a final project, **you need to implement at least two extensions**.\n",
    "\n",
    "### Download Albums\n",
    "\n",
    "Add support for downloading imgur albums.\n",
    "\n",
    "### Command Line Utility\n",
    "\n",
    "We saw in class that command-line arguments can be passed to Python scripts, and these arguments will be available through `sys.argv`. Modify your program so that it can be invoked with a single command-line argument representing the subreddit to scrape data from. So, `$ python wallscraper.py wallpaper` would download all the top wallpapers of the day, and `$ python wallscraper.py fffffffuuuuuuuuuuuu` would download all the top rage comics.\n",
    "\n",
    "### Configure your computer so that this script runs every hour/day/month\n",
    "\n",
    "Both OS X and Linux have ways to schedule a program to run every so often (Windows is harder). If you decide to do this option, talk with us. It's one of the coolest extensions, because you get awesome wallpapers over time, but it's also one of the hardest to get right. If you want to read up on your own, look up `launchd` and `cron`.\n",
    "\n",
    "### Programmatically set the highest-scoring wallpaper as your desktop wallpaper\n",
    "\n",
    "Both OS X and Linux have command-line tools to programmatically set your desktop background to be a specified file path (again, Windows is harder). In combination with the previous extension, you could have an automatically shifting desktop background of the internet's top trending wallpapers!\n",
    "\n",
    "### Support for Pagination\n",
    "\n",
    "We currently scrape only one page of Reddit data at a time. In the response data, there are pagination tokens `before` and `after` than can be used to scroll through pages and pages of reddit. Use these pagination tokens to search through arbitrarily many pages of a subreddit.\n",
    "\n",
    "### Wallpaper deduplication\n",
    "\n",
    "If we ever encounter the same wallpaper twice, we'll process the data twice, download it twice, etc. Implement a system that will eliminate image download duplication. You have freedom to implement this however you want.\n",
    "\n",
    "### Logging\n",
    "\n",
    "When you encounter errors, log the errors instead of printing an error message. Use the `logging` library.\n",
    "\n",
    "### Parallel Processing and Multithreading\n",
    "\n",
    "Extend the current download code to make use of Python's multiprocessing and multithreading primitives.\n",
    "\n",
    "## Grading\n",
    "\n",
    "You can use this assignment as a final project. If you're submitting for a final project, we'll grade on the following criteria:\n",
    "\n",
    "### Functionality\n",
    "\n",
    "We'll be testing your code on live Reddit data, so make sure it works on real subreddits and multireddits (as stated, we suggest `'/r/wallpapers+wallpaper+earthporn'`). There a lot of different ways you can take this assignment, so we'll be assessing functionality on a case-by-case basis. If your program handles errors gracefully and successfully downloads wallpapers from the internet, that's deserving of a &check;+! If the wallscraper is mostly correct, but fails on some inputs or crashes in certain conditions, that's a &check;. If the program *drastically* fails to either (1) connect to the internet and extract a list of top posts or (2) save posts to the filesystem, that would be a &check;-.\n",
    "\n",
    "### Style\n",
    "\n",
    "As always, your style grade is comprised of three main components:\n",
    "\n",
    "* **Pythonic practices:** Proper use of the Python tools and ways of thinking introduced in this class - using list comprehensions where appropriate, intelligent utilizing iterables/generators where appropriate, etc.\n",
    "* **Program design:** General programming style - decomposition, commenting, logic, algorithm design, etc.\n",
    "* **Python mechanics:** Basically everything covered in PEP8 - naming, spacing, parenthesizing, etc.\n",
    "\n",
    "### Extensions\n",
    "\n",
    "If you're using this assignment as a final project, we require that you implement **at least two extensions**. You're welcome to implement more or to come up with your own extensions, but you should run them by us first!\n",
    "\n",
    "## Credit\n",
    "\n",
    "*This assignment was written by Sam Redmond (@sredmond), rewritten by @psarin, and inspired by a late-night conversation with Eddie Wang (@eddiew). It wouldn't be possible without the careful review of Sherman Leung (@skleung) and course helpers David Slater (@dsslater), Brexton Pham (@bpham), Conner Smith (@csmith95), and Matt Mahowald (@mmahowald)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
